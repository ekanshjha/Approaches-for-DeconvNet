{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used is 100 images from VOC_2012 in the 'demo_data' folder of current directory.  \n",
    "75 images in train-data  \n",
    "25 images in test-data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packs loaded...\n"
     ]
    }
   ],
   "source": [
    "# Package imports\n",
    "from PIL import Image\n",
    "import _pickle as pkl\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "import glob\n",
    "import os\n",
    "#%matplotlib inline  \n",
    "print (\"Packs loaded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_list(imagepath, segmentpath):\n",
    "    '''\n",
    "    input: \n",
    "        imagepath -> path to images directory\n",
    "        Segmentpath -> path to segments directory\n",
    "    output: \n",
    "        list of paths to all the images and for segmentation ground-truth \n",
    "    '''\n",
    "    path1 = os.getcwd() + '/' + imagepath\n",
    "    path2 = os.getcwd() + '/' + segmentpath\n",
    "    imglist = glob.glob(path1 + '/*.jpg')\n",
    "    annotlist = glob.glob(path2 + '/*.png')\n",
    "    return imglist, annotlist\n",
    "\n",
    "def data_preprocess(imglist, annotlist, height=224, width=224, num_classes=2):\n",
    "    '''\n",
    "    input:\n",
    "        imglist, annotlist -> list of paths to all images, groung-truth\n",
    "    output:\n",
    "        Data -> 4-d array [number of images , height , width , 3]\n",
    "        Label -> 4-d array [number of annotations , height , width , 1]\n",
    "        LabelOneHot -> 4-d array [number of annotations , height , width , num_classes]\n",
    "    '''\n",
    "    Data = None\n",
    "    Label = None\n",
    "    LabelOneHot = None \n",
    "    for (f1, f2, i) in zip(imglist, annotlist, range(len(trainimglist))):\n",
    "        # image\n",
    "        img1 = Image.open(f1)\n",
    "        img1 = img1.resize((height, width))\n",
    "        rgb  = np.array(img1).reshape(1, height, width, 3)\n",
    "        # label\n",
    "        img2 = Image.open(f2)\n",
    "        img2 = img2.resize((height, width), Image.LINEAR)\n",
    "        label = np.array(img2).reshape(1, height, width, 1)\n",
    "        # Stack images and labels\n",
    "        if i == 0: \n",
    "            Data = rgb\n",
    "            Label = label\n",
    "        else:\n",
    "            Data = np.concatenate((Data, rgb), axis=0)\n",
    "            Label = np.concatenate((Label, label), axis=0)\n",
    "    \n",
    "    # Label for 'border' changed from '255' to '21' (new label) or to '0' (background)\n",
    "    Label[Label == 255] = 0\n",
    "    # remove other labels as well\n",
    "    Label[Label != 0] = 1\n",
    "\n",
    "    # Onehot-coded label\n",
    "    class_labels_tensor = tf.not_equal(Label, 0) #one class for others\n",
    "    background_labels_tensor = tf.equal(Label, 0) #zero class for background\n",
    "    ''' for more classes we can use onehotencoding from tf.contrib (one line code) '''\n",
    "    # Convert the boolean values into floats -- so that\n",
    "    # computations in cross-entropy loss is correct\n",
    "    bit_mask_class = tf.to_float(class_labels_tensor)\n",
    "    bit_mask_background = tf.to_float(background_labels_tensor)\n",
    "\n",
    "    LabelOneHot = (tf.concat(axis=3, values=[bit_mask_class,\n",
    "                                             bit_mask_background])).eval()\n",
    "    return Data, Label, LabelOneHot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 train images\n",
      "100 train annotations\n",
      "33 test images\n",
      "33 test annotations\n",
      "                               N * H *  W * C\n",
      "Training images:             (100, 224, 224, 3)\n",
      "Training annotation images:  (100, 224, 224, 1)\n",
      "Test images:                 (33, 224, 224, 3)\n",
      "Test annotations images:     (33, 224, 224, 1)\n",
      "\n",
      "Shape of 'trainLabelOneHot': (100, 224, 224, 2)\n",
      "Shape of 'testLabelOneHot' : (33, 224, 224, 2)\n",
      "\n",
      "* * * DATA PROCESSING COMPLETED * * *\n"
     ]
    }
   ],
   "source": [
    "# Location of the files\n",
    "\n",
    "train_image = 'demo_data/train_images/'\n",
    "train_segments = 'demo_data/train_segmented/'\n",
    "test_image = 'demo_data/test_images/'\n",
    "test_segments = 'demo_data/test_segmented/'\n",
    "\n",
    "trainimglist, trainannotlist = get_img_list(train_image, train_segments)\n",
    "trainimglist.sort()\n",
    "trainannotlist.sort()\n",
    "print (\"%d train images\" % (len(trainimglist)))\n",
    "print (\"%d train annotations\" % (len(trainannotlist)))\n",
    "\n",
    "\n",
    "testimglist, testannotlist = get_img_list(test_image, test_segments)\n",
    "testimglist.sort()\n",
    "testannotlist.sort()\n",
    "print (\"%d test images\" % (len(testimglist)))\n",
    "print (\"%d test annotations\" % (len(testannotlist)))\n",
    "\n",
    "trainData, trainLabel, trainLabelOneHot = data_preprocess(trainimglist, trainannotlist)\n",
    "testData, testLabel, testLabelOneHot = data_preprocess(testimglist, testannotlist)\n",
    "\n",
    "# Batch-dimensions\n",
    "print (\"                               N * H *  W * C\")\n",
    "print (\"Training images:            \", trainData.shape)\n",
    "print (\"Training annotation images: \", trainLabel.shape)\n",
    "print (\"Test images:                \", testData.shape)\n",
    "print (\"Test annotations images:    \", testLabel.shape)\n",
    "\n",
    "print ()\n",
    "print (\"Shape of 'trainLabelOneHot': %s\" % (trainLabelOneHot.shape,))\n",
    "print (\"Shape of 'testLabelOneHot' : %s\" % (testLabelOneHot.shape,))\n",
    "print (\"\\n* * * DATA PROCESSING COMPLETED * * *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image data     : 14.36  MB\n",
      "Train Annotation data: 4.79  MB\n",
      "Test Image data      : 4.74  MB\n",
      "Test Annotation data : 1.58  MB\n",
      "Train One-Hot Annotation data: 38.28  MB\n",
      "Test One-Hot Annotation data : 12.63  MB\n"
     ]
    }
   ],
   "source": [
    "# Memory consumption (by default np.array is FLOAT32 i.e. 32 bits)\n",
    "print (\"Train Image data     : %.2f\" % (trainData.nbytes / (1024*1024)), \" MB\")\n",
    "print (\"Train Annotation data: %.2f\" % (trainLabel.nbytes / (1024*1024)), \" MB\")\n",
    "print (\"Test Image data      : %.2f\" % (testData.nbytes / (1024*1024)), \" MB\")\n",
    "print (\"Test Annotation data : %.2f\" % (testLabel.nbytes / (1024*1024)), \" MB\")\n",
    "\n",
    "print (\"Train One-Hot Annotation data: %.2f\" % (trainLabelOneHot.nbytes / (1024*1024)), \" MB\")\n",
    "print (\"Test One-Hot Annotation data : %.2f\" % (testLabelOneHot.nbytes / (1024*1024)), \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "# Samples from data set\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "idx = 10\n",
    "# Sample_1\n",
    "ax11 = fig.add_subplot(gs[0, 0])\n",
    "ax11.imshow(trainData[idx])\n",
    "#ax11.axis('off')\n",
    "ax11.set_title('Original', fontsize=15)\n",
    "\n",
    "ax12 = fig.add_subplot(gs[0, 1])\n",
    "ax12.imshow(trainLabel[idx,:,:,0], cmap='gray')\n",
    "#ax12.axis('off')\n",
    "ax12.set_title('Segmented', fontsize=15)\n",
    "\n",
    "# Sample_2\n",
    "ax21 = fig.add_subplot(gs[1, 0])\n",
    "ax21.imshow(testData[idx])\n",
    "#ax21.axis('off')\n",
    "\n",
    "ax22 = fig.add_subplot(gs[1, 1])\n",
    "ax22.imshow(testLabel[idx,:,:,0], cmap='gray')\n",
    "#ax22.axis('off')\n",
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "nrclass = 2\n",
    "# Define placeholders\n",
    "x = tf.placeholder(tf.float32, [None, height, width, 3], name=\"X_image\")\n",
    "tf.summary.image(\"image\", x, 3)\n",
    "y = tf.placeholder(tf.float32, [None, height, width, nrclass], name=\"y_ground_truth\")\n",
    "keepprob = tf.placeholder(tf.float32)\n",
    "#keepprob yet to be added to the DeconvNet-------------------------------------\n",
    "\n",
    "# Kernels\n",
    "ksize = 3\n",
    "input_channels = 3\n",
    "\n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial,name=name, dtype=tf.float32)\n",
    "    \n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name, dtype=tf.float32)\n",
    "\n",
    "weights = {\n",
    "    \"wc1\" : weight_variable([3, 3, 3, 64], \"Weight_conv_1\"),\n",
    "    \"wc2\" : weight_variable([3, 3, 64, 64], \"Weight_conv_2\"),\n",
    "    \"wc3\" : weight_variable([3, 3, 64, 128], \"Weight_conv_3\"),\n",
    "    \"wc4\" : weight_variable([3, 3, 128, 128], \"Weight_conv_4\"),\n",
    "    \"wc5\" : weight_variable([3, 3, 128, 256], \"Weight_conv_5\"),\n",
    "    \"wc6\" : weight_variable([3, 3, 256, 256], \"Weight_conv_6\"),\n",
    "    \"wc7\" : weight_variable([3, 3, 256, 256], \"Weight_conv_7\"),\n",
    "    \"wc8\" : weight_variable([3, 3, 256, 512], \"Weight_conv_8\"),\n",
    "    \"wc9\" : weight_variable([3, 3, 512, 512], \"Weight_conv_9\"),\n",
    "    \"wc10\" : weight_variable([3, 3, 512, 512], \"Weight_conv_10\"),\n",
    "    \"wc11\" : weight_variable([3, 3, 512, 512], \"Weight_conv_11\"),\n",
    "    \"wc12\" : weight_variable([3, 3, 512, 512], \"Weight_conv_12\"),\n",
    "    \"wc13\" : weight_variable([3, 3, 512, 512], \"Weight_conv_13\"),\n",
    "    \"wc14\" : weight_variable([7, 7, 512, 2048], \"Weight_conv_14\"),\n",
    "    \"wc15\" : weight_variable([1, 1, 2048, 2048], \"Weight_conv_15\"),\n",
    "    \"wc16\" : weight_variable([7, 7, 512, 2048], \"Weight_conv_16\"),\n",
    "    \"wc17\" : weight_variable([3, 3, 512, 512], \"Weight_conv_17\"),\n",
    "    \"wc18\" : weight_variable([3, 3, 512, 512], \"Weight_conv_18\"),\n",
    "    \"wc19\" : weight_variable([3, 3, 512, 512], \"Weight_conv_19\"),\n",
    "    \"wc20\" : weight_variable([3, 3, 512, 512], \"Weight_conv_20\"),\n",
    "    \"wc21\" : weight_variable([3, 3, 512, 512], \"Weight_conv_21\"),\n",
    "    \"wc22\" : weight_variable([3, 3, 256, 512], \"Weight_conv_22\"),\n",
    "    \"wc23\" : weight_variable([3, 3, 256, 256], \"Weight_conv_23\"),\n",
    "    \"wc24\" : weight_variable([3, 3, 256, 256], \"Weight_conv_24\"),\n",
    "    \"wc25\" : weight_variable([3, 3, 128, 256], \"Weight_conv_25\"),\n",
    "    \"wc26\" : weight_variable([3, 3, 128, 128], \"Weight_conv_26\"),\n",
    "    \"wc27\" : weight_variable([3, 3, 64, 128], \"Weight_conv_27\"),\n",
    "    \"wc28\" : weight_variable([3, 3, 64, 64], \"Weight_conv_28\"),\n",
    "    \"wc29\" : weight_variable([3, 3, 64, 64], \"Weight_conv_29\"),\n",
    "    \"wc30\" : weight_variable([1, 1, 2, 64], \"Weight_conv_30\")\n",
    "    }\n",
    "\n",
    "biases = {\n",
    "    \"bc1\" : bias_variable([64], \"Bias_conv_1\"),\n",
    "    \"bc2\" : bias_variable([64], \"Bias_conv_2\"),\n",
    "    \"bc3\" : bias_variable([128], \"Bias_conv_3\"),\n",
    "    \"bc4\" : bias_variable([128], \"Bias_conv_4\"),\n",
    "    \"bc5\" : bias_variable([256], \"Bias_conv_5\"),\n",
    "    \"bc6\" : bias_variable([256], \"Bias_conv_6\"),\n",
    "    \"bc7\" : bias_variable([256], \"Bias_conv_7\"),\n",
    "    \"bc8\" : bias_variable([512], \"Bias_conv_8\"),\n",
    "    \"bc9\" : bias_variable([512], \"Bias_conv_9\"),\n",
    "    \"bc10\" : bias_variable([512], \"Bias_conv_10\"),\n",
    "    \"bc11\" : bias_variable([512], \"Bias_conv_11\"),\n",
    "    \"bc12\" : bias_variable([512], \"Bias_conv_12\"),\n",
    "    \"bc13\" : bias_variable([512], \"Bias_conv_13\"),\n",
    "    \"bc14\" : bias_variable([2048], \"Bias_conv_2\"),\n",
    "    \"bc15\" : bias_variable([2048], \"Bias_conv_3\"),\n",
    "    \"bc16\" : bias_variable([512], \"Bias_conv_16\"),\n",
    "    \"bc17\" : bias_variable([512], \"Bias_conv_17\"),\n",
    "    \"bc18\" : bias_variable([512], \"Bias_conv_18\"),\n",
    "    \"bc19\" : bias_variable([512], \"Bias_conv_19\"),\n",
    "    \"bc20\" : bias_variable([512], \"Bias_conv_20\"),\n",
    "    \"bc21\" : bias_variable([512], \"Bias_conv_21\"),\n",
    "    \"bc22\" : bias_variable([256], \"Bias_conv_22\"),\n",
    "    \"bc23\" : bias_variable([256], \"Bias_conv_23\"),\n",
    "    \"bc24\" : bias_variable([256], \"Bias_conv_24\"),\n",
    "    \"bc25\" : bias_variable([128], \"Bias_conv_25\"),\n",
    "    \"bc26\" : bias_variable([128], \"Bias_conv_26\"),\n",
    "    \"bc27\" : bias_variable([64], \"Bias_conv_27\"),\n",
    "    \"bc28\" : bias_variable([64], \"Bias_conv_28\"),\n",
    "    \"bc29\" : bias_variable([64], \"Bias_conv_29\"),\n",
    "    \"bc30\" : bias_variable([2], \"Bias_conv_30\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(x_input, w, b, strides=[1, 1, 1, 1], padding='SAME',name='conv_'):\n",
    "    with tf.name_scope(name):\n",
    "        conv = tf.nn.conv2d(x_input, w, strides=strides, padding=padding)\n",
    "        act = tf.nn.sigmoid(conv + b)\n",
    "        tf.summary.histogram('conv/weights', w)\n",
    "        tf.summary.histogram('conv/biases', b)\n",
    "        tf.summary.histogram('conv/activation', act)\n",
    "        return act\n",
    "\n",
    "def deconv_layer(x_input, w, b, strides=[1, 1, 1, 1], padding='SAME',name='deconv_'):\n",
    "    with tf.name_scope(name):\n",
    "        shape = tf.shape(x_input)\n",
    "        out_shape = [shape[0], shape[1], shape[2], w.get_shape().as_list()[2]]\n",
    "        deconv = tf.nn.conv2d_transpose(x_input, filter=w, output_shape=out_shape, strides=strides, padding=padding)    \n",
    "        act = tf.nn.sigmoid(deconv + b)\n",
    "        tf.summary.histogram('deconv/weights', w)\n",
    "        tf.summary.histogram('deconv/biases', b)\n",
    "        tf.summary.histogram('deconv/activation', act)\n",
    "        return act\n",
    "\n",
    "def pool_layer(x_input):\n",
    "    pool_maps, pool_argmax = tf.nn.max_pool_with_argmax(x_input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #summary of pool_maps to be added-----------------------------------------\n",
    "    return pool_maps, pool_argmax\n",
    "\n",
    "def unravel_argmax(argmax, shape):\n",
    "    output_list = [argmax // (shape[2]*shape[3]), argmax % (shape[2]*shape[3]) // shape[3]]\n",
    "    return tf.stack(output_list)\n",
    "\n",
    "def unpool_layer2x2_batch(bottom, argmax):\n",
    "    bottom_shape = tf.shape(bottom)\n",
    "    top_shape = [bottom_shape[0], bottom_shape[1]*2, bottom_shape[2]*2, bottom_shape[3]]\n",
    "\n",
    "    batch_size = top_shape[0]\n",
    "    height = top_shape[1]\n",
    "    width = top_shape[2]\n",
    "    channels = top_shape[3]\n",
    "\n",
    "    argmax_shape = tf.to_int64([batch_size, height, width, channels])\n",
    "    argmax = unravel_argmax(argmax, argmax_shape)\n",
    "\n",
    "    t1 = tf.to_int64(tf.range(channels))\n",
    "    t1 = tf.tile(t1, [batch_size*(width//2)*(height//2)])\n",
    "    t1 = tf.reshape(t1, [-1, channels])\n",
    "    t1 = tf.transpose(t1, perm=[1, 0])\n",
    "    t1 = tf.reshape(t1, [channels, batch_size, height//2, width//2, 1])\n",
    "    t1 = tf.transpose(t1, perm=[1, 0, 2, 3, 4])\n",
    "\n",
    "    t2 = tf.to_int64(tf.range(batch_size))\n",
    "    t2 = tf.tile(t2, [channels*(width//2)*(height//2)])\n",
    "    t2 = tf.reshape(t2, [-1, batch_size])\n",
    "    t2 = tf.transpose(t2, perm=[1, 0])\n",
    "    t2 = tf.reshape(t2, [batch_size, channels, height//2, width//2, 1])\n",
    "\n",
    "    t3 = tf.transpose(argmax, perm=[1, 4, 2, 3, 0])\n",
    "\n",
    "    t = tf.concat([t2, t3, t1], 4)\n",
    "    indices = tf.reshape(t, [(height//2)*(width//2)*channels*batch_size, 4])\n",
    "\n",
    "    x1 = tf.transpose(bottom, perm=[0, 3, 1, 2])\n",
    "    values = tf.reshape(x1, [-1])\n",
    "\n",
    "    delta = tf.SparseTensor(indices, values, tf.to_int64(top_shape))\n",
    "    unpool_maps = tf.sparse_tensor_to_dense(tf.sparse_reorder(delta))\n",
    "    #summary of unpool_maps to be added--------------------------------------\n",
    "    return unpool_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network ready\n"
     ]
    }
   ],
   "source": [
    "#------------Architecture---------------------------------------------------------------------------------------------------\n",
    "'''\n",
    "x           -----  (?, 224, 224, 3)\n",
    "conv_1_1    -----  (?, 224, 224, 64)\n",
    "conv_1_2    -----  (?, 224, 224, 64)\n",
    "pool_1      -----  (?, 112, 112, 64)\n",
    "conv_2_1    -----  (?, 112, 112, 128)\n",
    "conv_2_2    -----  (?, 112, 112, 128)\n",
    "pool_2      -----  (?, 56, 56, 128)\n",
    "conv_3_1    -----  (?, 56, 56, 256)\n",
    "conv_3_2    -----  (?, 56, 56, 256)\n",
    "conv_3_3    -----  (?, 56, 56, 256)\n",
    "pool_3      -----  (?, 28, 28, 256)\n",
    "conv_4_1    -----  (?, 28, 28, 512)\n",
    "conv_4_2    -----  (?, 28, 28, 512)\n",
    "conv_4_3    -----  (?, 28, 28, 512)\n",
    "pool_4      -----  (?, 14, 14, 512)\n",
    "conv_5_1    -----  (?, 14, 14, 512)\n",
    "conv_5_2    -----  (?, 14, 14, 512)\n",
    "conv_5_3    -----  (?, 14, 14, 512)\n",
    "pool_5      -----  (?, 7, 7, 512)\n",
    "fc_6        -----  (?, 1, 1, 2048)\n",
    "fc_7        -----  (?, 1, 1, 2048)\n",
    "deconv_fc_6 -----  (?, 7, 7, 512)\n",
    "unpool_5    -----  (?, 14, 14, 512)\n",
    "deconv_5_3  -----  (?, 14, 14, 512)\n",
    "deconv_5_2  -----  (?, 14, 14, 512)\n",
    "deconv_5_1  -----  (?, 14, 14, 512)\n",
    "unpool_4    -----  (?, 28, 28, 512)\n",
    "deconv_4_3  -----  (?, 28, 28, 512)\n",
    "deconv_4_2  -----  (?, 28, 28, 512)\n",
    "deconv_4_1  -----  (?, 28, 28, 256)\n",
    "unpool_3    -----  (?, 56, 56, 256)\n",
    "deconv_3_3  -----  (?, 56, 56, 256)\n",
    "deconv_3_2  -----  (?, 56, 56, 256)\n",
    "deconv_3_1  -----  (?, 56, 56, 128)\n",
    "unpool_2    -----  (?, 112, 112, 128)\n",
    "deconv_2_2  -----  (?, 112, 112, 128)\n",
    "deconv_2_1  -----  (?, 112, 112, 64)\n",
    "unpool_1    -----  (?, 224, 224, 64)\n",
    "deconv_1_2  -----  (?, 224, 224, 64)\n",
    "deconv_1_1  -----  (?, 224, 224, 64)\n",
    "pred        -----  (?, 224, 224, 2)\n",
    "''' \n",
    "def decon_net(x, weights, biases):\n",
    "    # dropout layers need to be added------------------------------------------\n",
    "    conv_1_1 = conv_layer(x, weights['wc1'], biases['bc1'])                      \n",
    "    conv_1_2 = conv_layer(conv_1_1, weights['wc2'], biases['bc2'])\n",
    "    \n",
    "    pool_1, pool_1_argmax = pool_layer(conv_1_2)\n",
    "    \n",
    "    conv_2_1 = conv_layer(pool_1, weights['wc3'], biases['bc3'])\n",
    "    conv_2_2 = conv_layer(conv_2_1, weights['wc4'], biases['bc4'])\n",
    "    \n",
    "    pool_2, pool_2_argmax = pool_layer(conv_2_2)\n",
    "    \n",
    "    conv_3_1 = conv_layer(pool_2, weights['wc5'], biases['bc5'])\n",
    "    conv_3_2 = conv_layer(conv_3_1, weights['wc6'], biases['bc6'])\n",
    "    conv_3_3 = conv_layer(conv_3_2, weights['wc7'], biases['bc7'])\n",
    "    \n",
    "    pool_3, pool_3_argmax = pool_layer(conv_3_3)\n",
    "    \n",
    "    conv_4_1 = conv_layer(pool_3, weights['wc8'], biases['bc8'])\n",
    "    conv_4_2 = conv_layer(conv_4_1, weights['wc9'], biases['bc9'])\n",
    "    conv_4_3 = conv_layer(conv_4_2, weights['wc10'], biases['bc10'])\n",
    "    \n",
    "    pool_4, pool_4_argmax = pool_layer(conv_4_3)\n",
    "    \n",
    "    conv_5_1 = conv_layer(pool_4, weights['wc11'], biases['bc11'])\n",
    "    conv_5_2 = conv_layer(conv_5_1, weights['wc12'], biases['bc12'])\n",
    "    conv_5_3 = conv_layer(conv_5_2, weights['wc13'], biases['bc13'])\n",
    "    \n",
    "    pool_5, pool_5_argmax = pool_layer(conv_5_3)\n",
    "    \n",
    "    fc_6 = conv_layer(pool_5, weights['wc14'], biases['bc14'])\n",
    "    fc_7 = conv_layer(fc_6, weights['wc15'], biases['bc15'])\n",
    "    \n",
    "    deconv_fc_6 = deconv_layer(fc_7, weights['wc16'], biases['bc16'])\n",
    "    \n",
    "    unpool_5 = unpool_layer2x2_batch(deconv_fc_6, pool_5_argmax)\n",
    "    \n",
    "    deconv_5_3 = deconv_layer(unpool_5, weights['wc17'], biases['bc17'])\n",
    "    deconv_5_2 = deconv_layer(deconv_5_3, weights['wc18'], biases['bc18'])\n",
    "    deconv_5_1 = deconv_layer(deconv_5_2, weights['wc19'], biases['bc19'])\n",
    "    \n",
    "    unpool_4 = unpool_layer2x2_batch(deconv_5_1, pool_4_argmax)\n",
    "    \n",
    "    deconv_4_3 = deconv_layer(unpool_4, weights['wc20'], biases['bc20'])\n",
    "    deconv_4_2 = deconv_layer(deconv_4_3, weights['wc21'], biases['bc21'])\n",
    "    deconv_4_1 = deconv_layer(deconv_4_2, weights['wc22'], biases['bc22'])\n",
    "    \n",
    "    unpool_3 = unpool_layer2x2_batch(deconv_4_1, pool_3_argmax)\n",
    "    \n",
    "    deconv_3_3 = deconv_layer(unpool_3, weights['wc23'], biases['bc23'])\n",
    "    deconv_3_2 = deconv_layer(deconv_3_3, weights['wc24'], biases['bc24'])\n",
    "    deconv_3_1 = deconv_layer(deconv_3_2, weights['wc25'], biases['bc25'])\n",
    "    \n",
    "    unpool_2 = unpool_layer2x2_batch(deconv_3_1, pool_2_argmax)\n",
    "    \n",
    "    deconv_2_2 = deconv_layer(unpool_2, weights['wc26'], biases['bc26'])\n",
    "    deconv_2_1 = deconv_layer(deconv_2_2, weights['wc27'], biases['bc27'])\n",
    "    \n",
    "    unpool_1 = unpool_layer2x2_batch(deconv_2_1, pool_1_argmax)\n",
    "    \n",
    "    deconv_1_2 = deconv_layer(unpool_1, weights['wc28'], biases['bc28'])\n",
    "    deconv_1_1 = deconv_layer(deconv_1_2, weights['wc29'], biases['bc29'])\n",
    "    \n",
    "    pred = deconv_layer(deconv_1_1, weights['wc30'], biases['bc30'])\n",
    "    return pred\n",
    "\n",
    "\n",
    "pred = decon_net(x, weights, biases)\n",
    "\n",
    "# 'predmax' is the predicted-segmentation of input image (Grayscale)\n",
    "# it is tensor of type [N x H x W x 1]\n",
    "predmax = tf.argmax(pred, 3) \n",
    "#summary of predmax needs to be added ---------------------------------------\n",
    "\n",
    "\n",
    "# 'ymax' is the ground-truth-segmentation of input image (Grayscale)\n",
    "# (equal to original annotations)\n",
    "# it is tensor of type [N x H x W x 1]\n",
    "ymax = tf.argmax(y, 3)\n",
    "\n",
    "lin_pred = tf.reshape(pred, shape=[-1, nrclass])\n",
    "lin_y = tf.reshape(y, shape=[-1, nrclass])\n",
    "\n",
    "with tf.name_scope(\"Cross_entropy\"):\n",
    "    # Set it to false if you want to use customized method for cross_entropy\n",
    "    if False:\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=lin_pred, labels=lin_y))\n",
    "    else:\n",
    "        n = 0.7\n",
    "        pred = tf.nn.softmax(pred)\n",
    "        cross_entropy = - n * (tf.reduce_sum(lin_y * tf.log(lin_pred))) - (1-n) * (tf.reduce_sum((1-lin_y) * tf.log(1-lin_pred)))\n",
    "    tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "    \n",
    "with tf.name_scope(\"Train_step\"):\n",
    "    optm = tf.train.AdamOptimizer(0.000001).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct = tf.equal(ymax, predmax) \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "print('Network ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write figure to disk\n",
    "def save_figure(plot_dir, epoch, name_prefix, img_x, img_y, img_pred, img_err):\n",
    "    filename = name_prefix + str(epoch)\n",
    "    plt.figure(figsize=(7, 7)) \n",
    "    plt.subplot(2, 2, 1); plt.imshow(img_x); plt.title('Input')\n",
    "    plt.subplot(2, 2, 2); plt.imshow(img_y, cmap='gray'); plt.title('Ground truth')\n",
    "    plt.subplot(2, 2, 3); plt.imshow(img_pred, cmap='gray'); plt.title('[Train] Prediction')\n",
    "    plt.subplot(2, 2, 4); plt.imshow(np.abs(img_err) > 0.5); plt.title('Error')\n",
    "    plt.savefig(os.path.join(PLOT_DIR, '{}.png'.format(filename)), bbox_inches='tight')\n",
    "    \n",
    "# Generate sample results from test and train data\n",
    "def generate_samples(epoch, data, dataLabel_onehot, session, height, width):\n",
    "    # Feed data\n",
    "    index = np.random.randint(data.shape[0])\n",
    "    batchData = data[index:index+1]\n",
    "    batchLabel = dataLabel_onehot[index:index+1]\n",
    "    # Process Data\n",
    "    predMaxOut, yMaxOut = session.run([predmax, ymax], feed_dict={x: batchData, y: batchLabel})\n",
    "    # Prepare Output\n",
    "    refimg = data[index, :, :, :].reshape(height, width, 3)\n",
    "    gtimg = yMaxOut[0, :, :].reshape(height, width)\n",
    "    predimg = predMaxOut[0, :, :].reshape(height, width)\n",
    "    errimg = gtimg - predimg\n",
    "    return refimg, gtimg, predimg, errimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: None\n",
      "Couldn't find checkpoint to restore from. Starting over.\n",
      "**** Iteration:  0\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319352.0 \tTrain Accuracy:  0.533893\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319356.0 \tTrain Accuracy:  0.534491\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319395.0 \tTrain Accuracy:  0.530712\n",
      "Test Cross-Entropy:  319427.0 Test Accuracy:  0.526782\n",
      "**** Iteration:  1\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319362.0 \tTrain Accuracy:  0.532677\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319405.0 \tTrain Accuracy:  0.529191\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319407.0 \tTrain Accuracy:  0.528037\n",
      "**** Iteration:  2\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319365.0 \tTrain Accuracy:  0.532745\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319393.0 \tTrain Accuracy:  0.530473\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319424.0 \tTrain Accuracy:  0.529018\n",
      "**** Iteration:  3\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319446.0 \tTrain Accuracy:  0.525847\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319378.0 \tTrain Accuracy:  0.531621\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319430.0 \tTrain Accuracy:  0.528143\n",
      "**** Iteration:  4\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319364.0 \tTrain Accuracy:  0.53305\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319397.0 \tTrain Accuracy:  0.530455\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319399.0 \tTrain Accuracy:  0.529215\n",
      "**** Iteration:  5\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319391.0 \tTrain Accuracy:  0.531557\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319419.0 \tTrain Accuracy:  0.529225\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319367.0 \tTrain Accuracy:  0.533127\n",
      "Test Cross-Entropy:  319427.0 Test Accuracy:  0.526782\n",
      "**** Iteration:  6\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319400.0 \tTrain Accuracy:  0.530545\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319436.0 \tTrain Accuracy:  0.527416\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319391.0 \tTrain Accuracy:  0.530042\n",
      "**** Iteration:  7\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319387.0 \tTrain Accuracy:  0.530935\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319450.0 \tTrain Accuracy:  0.526907\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319375.0 \tTrain Accuracy:  0.530656\n",
      "**** Iteration:  8\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319397.0 \tTrain Accuracy:  0.530465\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319401.0 \tTrain Accuracy:  0.529831\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319402.0 \tTrain Accuracy:  0.52961\n",
      "**** Iteration:  9\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319417.0 \tTrain Accuracy:  0.528691\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319397.0 \tTrain Accuracy:  0.530957\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319423.0 \tTrain Accuracy:  0.528956\n",
      "**** Iteration:  10\n",
      "\tMini-Batch:  0 \tTrain Cross-Entropy:  319440.0 \tTrain Accuracy:  0.527013\n",
      "\tMini-Batch:  1 \tTrain Cross-Entropy:  319401.0 \tTrain Accuracy:  0.529963\n",
      "\tMini-Batch:  2 \tTrain Cross-Entropy:  319391.0 \tTrain Accuracy:  0.531459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siraz\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "LOGDIR = \"tmp/deconv_net/\"\n",
    "PLOT_DIR = \"plots\"\n",
    "batch_size = 10\n",
    "n_epochs = 100\n",
    "ntrain = trainData.shape[0]\n",
    "num_batch = 3#int(ntrain/batch_size)+1\n",
    "resumeTraining = True\n",
    "\n",
    "# it is interactive session\n",
    "# you need to initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "checkpoint = tf.train.latest_checkpoint(\"saved_ckpt/\")\n",
    "print (\"checkpoint: %s\" % (checkpoint))\n",
    "if  checkpoint:\n",
    "    print (\"Restoring from checkpoint\", checkpoint)\n",
    "    saver.restore(sess, checkpoint)\n",
    "else:\n",
    "    print (\"Couldn't find checkpoint to restore from. Starting over.\")\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(LOGDIR, graph=tf.get_default_graph())\n",
    "\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    print (\"**** Iteration: \", epoch_i)\n",
    "\n",
    "    for mini_epoch_i in range(num_batch):\n",
    "        # creating a 'mini-batch'\n",
    "        randidx = np.random.randint(ntrain, size=batch_size)\n",
    "        batchData = trainData[randidx]\n",
    "        batchLabel = trainLabelOneHot[randidx]\n",
    "        \n",
    "        s, cross_entropy_i, accuracy_i = sess.run([merged_summary, cross_entropy, accuracy], \n",
    "                                                  feed_dict={x: batchData, y: batchLabel})\n",
    "        writer.add_summary(s, mini_epoch_i + num_batch*epoch_i)\n",
    "        \n",
    "        if mini_epoch_i % 1 == 0:\n",
    "            print (\"\\tMini-Batch: \", mini_epoch_i, \n",
    "                   \"\\tTrain Cross-Entropy: \", cross_entropy_i, \n",
    "                   \"\\tTrain Accuracy: \", accuracy_i)\n",
    "                                      \n",
    "    # Sample results\n",
    "    img = generate_samples(epoch_i, trainData, trainLabelOneHot, sess, height, width)\n",
    "    save_figure(PLOT_DIR, epoch_i, \"train-\", img[0], img[1], img[2], img[3])\n",
    "\n",
    "    img = generate_samples(epoch_i, testData, testLabelOneHot, sess, height, width)\n",
    "    save_figure(PLOT_DIR, epoch_i, \"test-\", img[0], img[1], img[2], img[3])\n",
    "\n",
    "\n",
    "    # Evaluate TEST-DATA after every 5 iterations\n",
    "    # (only first 10 images of test set are taken)\n",
    "    if epoch_i % 5 == 0:\n",
    "        test_cross_entropy, test_accuracy = sess.run([cross_entropy, accuracy], \n",
    "                                                     feed_dict={x: testData[0:10], \n",
    "                                                                y: testLabelOneHot[0:10]})\n",
    "        print (\"Test Cross-Entropy: \", test_cross_entropy, \n",
    "               \"Test Accuracy: \", test_accuracy)\n",
    "\n",
    "    # tf.Save\n",
    "    saver.save(sess, 'saved_ckpt/', global_step = epoch_i)\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
